{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment 5 - NN_from_scratch_MNIST.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksz_N21WOQPp",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 5 - Neural Networks Implementation (due: Wed April 15th)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhNbH5I2OQPw",
        "colab_type": "text"
      },
      "source": [
        "This notebook show neural networks implemnetation on the MNIST hand-written dataset.  Adapted from a version published by https://github.com/jdwittenauer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82U7cO5OQP0",
        "colab_type": "text"
      },
      "source": [
        "We are going go over the neural network architecture discussed in class. This includes the feed-forward neural network - from input through the hidden layers to the out put and adjusting the weights as to minimize the loss step-by-step through each layer called **backpropagation.**  We'll implement both unregularized and regularized versions of the neural network cost function and gradient computation in the backpropagation function.  We'll also implement random weight initialization and a method to use the network to make predictions.\n",
        "\n",
        "Instead of manually going through gradient descent iterations, we'll simply use a package available for minimizing the backpropogation function.\n",
        "\n",
        "The MNIST data set is available in different sizes and formats from different sources.  We will use the one made avialble here: \"ex3data1.mat\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2BB4V2SOQP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPECwx9zOplv",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "38284f26-17e7-4583-f8e9-7beef4b7956d"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10753a66-9ee4-4d26-84f4-1a01d2accf0f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-10753a66-9ee4-4d26-84f4-1a01d2accf0f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving y_df.pkl to y_df.pkl\n",
            "Saving X_df.pkl to X_df.pkl\n",
            "Saving ex3data1.mat to ex3data1.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtJrxpA1OQQA",
        "colab_type": "text"
      },
      "source": [
        "The data we are loading is a dictionary that contains both input data and output.  The input data is a 2D numpy array X contains 5000 grayscale images of handwritten digits of 0, 1, 2, 3, ... 9.  Each raw of 400 entries between 0 and 255 that are grayscale intensity values of 20X20 pixel representation of a handwritten digit that is unrolled into a single vector.\n",
        "\n",
        "The output y contains the corresponding labels of 0,1,2, ..., 9.  \n",
        "\n",
        "Note that in the original convention followed for this dataset: 0 is mapped to 10, while all other labels are exactly the digit they represent.  Unlike in many other programming languages Python entries start at 0 and so we will change 10 back to 0 as it ought to be.\n",
        "\n",
        "**Multiclass classification:** You may notice that in our logistic regression models the output could only be one of two values: 0 or 1.  However what we are attempting here is an example of multiclass classification: outputs of more than two values - 10 in this case.  Neural networks facilitates this easily. In fact what we will allow now is the terminal layer to consist of multiple activations: 10 in this problem.  We use a function called softmax to make the outputs to be probabilties that adds upto 1.  The predicted value will correspond to the activation node with highest probabilty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLq-LDAcOQQC",
        "colab_type": "text"
      },
      "source": [
        "### Exercise #1: \n",
        "\n",
        "Load the data following the following five cells or by running the 6th cell\n",
        "\n",
        "- you should have the X and y from data in the dataframes X_df and y_df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZFg5YUuOQQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d34efa81-d557-4665-fbad-0e36ca168e86"
      },
      "source": [
        "##This is a dictionary that contains both input data and output.\n",
        "data = loadmat('ex3data1.mat')\n",
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " '__globals__': [],\n",
              " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
              " '__version__': '1.0',\n",
              " 'y': array([[10],\n",
              "        [10],\n",
              "        [10],\n",
              "        ...,\n",
              "        [ 9],\n",
              "        [ 9],\n",
              "        [ 9]], dtype=uint8)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgS6i1GOQQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "348e9f6d-27db-470b-9683-9c92a7b4c938"
      },
      "source": [
        "data['X'].shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dcfDzyJOQQS",
        "colab_type": "text"
      },
      "source": [
        "This is the data we will be using through out, let's create some useful variables up-front."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-71KQGSlOQQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data['X']\n",
        "y = data['y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trohDuaEOQQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let us set the values '10' back what it ought to be: 0\n",
        "y[y==10]=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWUuwd9cOQQb",
        "colab_type": "text"
      },
      "source": [
        "#### Let us convert these data sets into dataframes so we can pickle them and move them around without losing integrity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp1stwfSOQQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_df = pd.DataFrame(X)\n",
        "y_df = pd.DataFrame(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrPKVBIQOQQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "X_df = pd.read_pickle('X_df.pkl')\n",
        "y_df = pd.read_pickle('y_df.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioDx6tBvOQQj",
        "colab_type": "text"
      },
      "source": [
        "### Exercise #2:\n",
        "\n",
        "Display **20** randomly selected images from X_df and display them in two rows by appropriately changing the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bebkULnROQQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "0add8025-5b9a-49f1-9371-82c602634f8c"
      },
      "source": [
        "#Let us quickly checkout what is in these data sets:\n",
        "sample = np.random.randint(0,len(X), 20)\n",
        "X_samples = [X[i,:] for i in sample]\n",
        "\n",
        "#plot the digits\n",
        "i = 0\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=10)\n",
        "for row in ax:\n",
        "    for col in row:\n",
        "        col.imshow(X_samples[i].reshape(20,20))\n",
        "        i += 1\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC1CAYAAABGS6SMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUcCLKKKIIs5S/H/X6hGQ1LNnvwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnzZEmSmOQQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3d046184-df12-4558-ed6c-d7426d34ed89"
      },
      "source": [
        "#Checkout their labels\n",
        "y[sample].flatten()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 3, 4, 5, 0, 6, 7, 2, 6, 5, 6, 7, 1, 6, 8, 4, 1, 5, 7, 8],\n",
              "      dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKhFPZQXOQQr",
        "colab_type": "text"
      },
      "source": [
        "**One hot encoding:** Now we need to turn the label array into what is called one-hot-encoded matrix.  The one-hot-matrix is a representation of representation of a n-dimensional vector of values 0, 1, 2, ..., 9 as in here into 10Xn matrix where each column will have all zeros except exactly one 1 in the position corresponding to the value the column represents.  For example 7 will be a column where all entries except the 7th are all zeros and the 7th is a one.\n",
        "\n",
        "One-hot-encoding is a way to represent a vector with binary entries alone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk5QSd8qOQQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff79dc73-616d-42f5-d137-45ce91669a0d"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "y_onehot.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGc7ONtCOQQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be49daf0-9f63-496a-dfdd-6cce823cb100"
      },
      "source": [
        "#Here is an example of an y entry and its one-hot-encoding\n",
        "y[0], y_onehot[0,:]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0], dtype=uint8), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyOljN6YOQQz",
        "colab_type": "text"
      },
      "source": [
        "The neural network we're going to build for this exercise has an input layer matching the size of our instance data (400 + the bias unit), a hidden layer with 25 units (26 with the bias unit), and an output layer with 10 units corresponding to our one-hot encoding for the class labels. \n",
        "\n",
        "The first piece we need to implement is a cost function to evaluate the loss for a given set of network parameters.  The source mathematical function is in the exercise text (and looks pretty intimidating).  Here are the functions required to compute the cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Pkvcp_OQQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaMOl5RgOQQ3",
        "colab_type": "text"
      },
      "source": [
        "## Forward propogation and Cost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlreIRqyOQQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Forward propogation: This takes us through one sweep from input through one hidden layer to the outputs\n",
        "def forward_propagate(X, theta1, theta2):\n",
        "    m = X.shape[0]\n",
        "    \n",
        "    #First insert a a row of ones into the X data - 0th column\n",
        "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
        "    \n",
        "    #Caluculate the X*Theta\n",
        "    z2 = a1 * theta1.T\n",
        "    \n",
        "    #now apply the sigmoid function and then insert a column of ones\n",
        "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
        "    \n",
        "    #Calculate X*Theta\n",
        "    z3 = a2 * theta2.T\n",
        "    \n",
        "    #The outputs\n",
        "    h = sigmoid(z3)\n",
        "    \n",
        "    #Now return all the calculates values that we need for calculating and then backpropagation\n",
        "    return a1, z2, a2, z3, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp60dzcCOQQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Cost\n",
        "def cost(params, input_size, hidden_size1, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    n1 = hidden_size1 * (input_size + 1)\n",
        "    n2 = n1 + num_labels * (hidden_size1 + 1)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    theta1 = np.matrix(np.reshape(params[:n1], (hidden_size1, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[n1:n2], (num_labels, (hidden_size1 + 1))))\n",
        "\n",
        "    \n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
        "    \n",
        "    # compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = - np.multiply(y[i,:], np.log(h[i,:]))\n",
        "        second_term = - (np.multiply((1 - y[i,:]), np.log(1 - h[i,:])))\n",
        "        J += np.sum(first_term + second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8FfTRjYOQQ_",
        "colab_type": "text"
      },
      "source": [
        "We've used the sigmoid function before so that's not new.  The forward-propagate function computes the hypothesis for each training instance given the current parameters.  It's output shape should match the same of our one-hot encoding for y.  We can test this real quick to convince ourselves that it's working as expected (the intermediate steps are also returned as these will be useful later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGOr2_9yOQRA",
        "colab_type": "text"
      },
      "source": [
        "### Exercise #3\n",
        "\n",
        "Work the rest of this application (except for exercise 4 later) using hidden_size2 set to 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNm-EwQcOQRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac5e757c-6a35-4350-e848-45ef45f0cf28"
      },
      "source": [
        "# initial setup\n",
        "input_size = 400\n",
        "hidden_size1 = 50\n",
        "num_labels = 10\n",
        "learning_rate = 1\n",
        "\n",
        "# randomly initialize a parameter array of the size of the full network's parameters\n",
        "params = (np.random.random(size=hidden_size1 * (input_size + 1) + num_labels * (hidden_size1 + 1)) - 0.5) * 0.25\n",
        "\n",
        "m = X.shape[0]\n",
        "X = np.matrix(X)\n",
        "y = np.matrix(y)\n",
        "\n",
        "# unravel the parameter array into parameter matrices for each layer\n",
        "theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):], (num_labels, (hidden_size1 + 1))))\n",
        "\n",
        "theta1.shape, theta2.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50, 401), (10, 51))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wWU_reaOQRH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fab286d-2e0b-43b8-c5a4-f30d4be7d34b"
      },
      "source": [
        "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
        "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 401), (5000, 50), (5000, 51), (5000, 10), (5000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9byyuDCOQRL",
        "colab_type": "text"
      },
      "source": [
        "The cost function, after computing the hypothesis matrix h, applies the cost equation to compute the total error between y and h."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbYzW1PzOQRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1d117be-5dfa-4fe8-92e9-18117e1c7f11"
      },
      "source": [
        "cost(params, input_size, hidden_size1, num_labels, X, y_onehot, learning_rate)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.176108966190751"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vYFUztgOQRP",
        "colab_type": "text"
      },
      "source": [
        "## Forward propogation with regularization and Cost\n",
        "\n",
        "Our next step is to add regularization to the cost function.  If you're following along in the exercise text and thought the last equation looked ugly, this one looks REALLY ugly.  It's actually not as complicated as it looks though - in fact, the regularization term is simply an addition to the cost we already computed.  Here's the revised cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6wGYOtnOQRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(params, input_size, hidden_size1, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    \n",
        "    n1 = hidden_size1 * (input_size + 1)\n",
        "    n2 = n1 + num_labels * (hidden_size1 + 1)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    theta1 = np.matrix(np.reshape(params[:n1], (hidden_size1, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[n1:n2], (num_labels, (hidden_size1 + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
        "    \n",
        "    # compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
        "    \n",
        "    return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3-ukrDOQRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11510ca5-cd90-4bd9-9d76-934cb20b5c87"
      },
      "source": [
        "cost(params, input_size, hidden_size1, num_labels, X, y_onehot, learning_rate)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.186795909823177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9GAu9bVOQRW",
        "colab_type": "text"
      },
      "source": [
        "Next up is the backpropagation algorithm.  Backpropagation computes the parameter updates that will reduce the error of the network on the training data.  The first thing we need is a function that computes the gradient of the sigmoid function we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCyWMVigOQRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_gradient(z):\n",
        "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0WrW3L4OQRZ",
        "colab_type": "text"
      },
      "source": [
        "## Backpropogation\n",
        "\n",
        "Now we're ready to implement backpropagation to compute the gradients.  Since the computations required for backpropagation are a superset of those required in the cost function, we're actually going to extend the cost function to also perform backpropagation and return both the cost and the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amhN16htOQRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(params, input_size, hidden_size1, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):], (num_labels, (hidden_size1 + 1))))\n",
        "    \n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
        "    \n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(theta2.shape)  # (10, 26)\n",
        "    \n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
        "    \n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]  # (1, 401)\n",
        "        z2t = z2[t,:]  # (1, 25)\n",
        "        a2t = a2[t,:]  # (1, 26)\n",
        "        ht = h[t,:]  # (1, 10)\n",
        "        yt = y[t,:]  # (1, 10)\n",
        "        \n",
        "        d3t = ht - yt  # (1, 10)\n",
        "        \n",
        "        #insert a one in first position for bias\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)\n",
        "        d2t = np.multiply((theta2.T * d3t.T).T, sigmoid_gradient(z2t))  # (1, 26)\n",
        "        \n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + d3t.T * a2t\n",
        "        \n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    \n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
        "    \n",
        "    return J, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXxsiN4OQRd",
        "colab_type": "text"
      },
      "source": [
        "The hardest part of the backprop computation (other than understanding WHY we're doing all these calculations) is getting the matrix dimensions right.  By the way, if you find it confusing when to use A * B vs. np.multiply(A, B), you're not alone.  Basically the former is a matrix multiplication and the latter is an element-wise multiplication (unless A or B is a scalar value, in which case it doesn't matter).  I wish there was a more concise syntax for this (maybe there is and I'm just not aware of it).\n",
        "\n",
        "Anyway, let's test it out to make sure the function returns what we're expecting it to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx-U2p_1OQRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f772d402-0307-4031-ea8a-7c438821ae17"
      },
      "source": [
        "J, grad = backprop(params, input_size, hidden_size1, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7.186795909823177, (20560,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_TAiEFAOQRi",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4:\n",
        "\n",
        "1.  Split the datasets X_df and y_onehot into 80% for training and the rest for testing using:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "2. Then use only the train datasets to train (in the next cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNoP-TdIOQRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "513fb778-a3c2-4780-c53e-415d2e436862"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y_onehot,test_size=0.20,random_state=23)\n",
        "\n",
        "# minimize the objective function\n",
        "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size1, num_labels, X_train, y_train, learning_rate), \n",
        "                method='TNC', jac=True, options={'maxiter': 250})\n",
        "fmin"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     fun: 0.6447026965253835\n",
              "     jac: array([-3.52686805e-04,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "       -2.65683093e-05, -6.06850510e-05, -2.26970570e-04])\n",
              " message: 'Max. number of function evaluations reached'\n",
              "    nfev: 250\n",
              "     nit: 14\n",
              "  status: 3\n",
              " success: False\n",
              "       x: array([-0.16000625, -0.1035491 , -0.03465699, ...,  0.35022924,\n",
              "       -1.71524015,  1.93676715])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiqYAMstOQRm",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5:\n",
        "\n",
        "1.  Now use only the test datasets to predict (in the next cell\n",
        "2.  And calclate the accuray by comparing to the test labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8WOPNxGOQRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20bb4c40-7db7-487a-a6df-8d2f637f151e"
      },
      "source": [
        "X = np.matrix(X)\n",
        "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(fmin.x[hidden_size1 * (input_size + 1):], (num_labels, (hidden_size1 + 1))))\n",
        "\n",
        "a1, z2, a2, z3, h = forward_propagate(X_test, theta1, theta2)\n",
        "y_pred = np.array(np.argmax(h, axis=1))\n",
        "y_pred"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3],\n",
              "       [1],\n",
              "       [6],\n",
              "       [4],\n",
              "       [0],\n",
              "       [3],\n",
              "       [8],\n",
              "       [6],\n",
              "       [9],\n",
              "       [4],\n",
              "       [5],\n",
              "       [8],\n",
              "       [8],\n",
              "       [9],\n",
              "       [9],\n",
              "       [0],\n",
              "       [0],\n",
              "       [5],\n",
              "       [8],\n",
              "       [7],\n",
              "       [7],\n",
              "       [4],\n",
              "       [9],\n",
              "       [3],\n",
              "       [7],\n",
              "       [0],\n",
              "       [7],\n",
              "       [1],\n",
              "       [0],\n",
              "       [2],\n",
              "       [4],\n",
              "       [9],\n",
              "       [8],\n",
              "       [6],\n",
              "       [4],\n",
              "       [0],\n",
              "       [0],\n",
              "       [7],\n",
              "       [4],\n",
              "       [4],\n",
              "       [4],\n",
              "       [1],\n",
              "       [8],\n",
              "       [9],\n",
              "       [5],\n",
              "       [4],\n",
              "       [8],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [7],\n",
              "       [7],\n",
              "       [6],\n",
              "       [7],\n",
              "       [9],\n",
              "       [2],\n",
              "       [7],\n",
              "       [1],\n",
              "       [5],\n",
              "       [7],\n",
              "       [8],\n",
              "       [7],\n",
              "       [0],\n",
              "       [7],\n",
              "       [1],\n",
              "       [5],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [3],\n",
              "       [8],\n",
              "       [8],\n",
              "       [5],\n",
              "       [8],\n",
              "       [1],\n",
              "       [6],\n",
              "       [8],\n",
              "       [2],\n",
              "       [5],\n",
              "       [8],\n",
              "       [2],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [9],\n",
              "       [7],\n",
              "       [8],\n",
              "       [2],\n",
              "       [1],\n",
              "       [3],\n",
              "       [6],\n",
              "       [4],\n",
              "       [6],\n",
              "       [6],\n",
              "       [4],\n",
              "       [5],\n",
              "       [7],\n",
              "       [9],\n",
              "       [2],\n",
              "       [5],\n",
              "       [9],\n",
              "       [9],\n",
              "       [0],\n",
              "       [1],\n",
              "       [6],\n",
              "       [6],\n",
              "       [0],\n",
              "       [6],\n",
              "       [0],\n",
              "       [5],\n",
              "       [7],\n",
              "       [0],\n",
              "       [4],\n",
              "       [0],\n",
              "       [5],\n",
              "       [7],\n",
              "       [3],\n",
              "       [9],\n",
              "       [5],\n",
              "       [5],\n",
              "       [8],\n",
              "       [9],\n",
              "       [7],\n",
              "       [2],\n",
              "       [2],\n",
              "       [0],\n",
              "       [8],\n",
              "       [2],\n",
              "       [5],\n",
              "       [8],\n",
              "       [1],\n",
              "       [3],\n",
              "       [3],\n",
              "       [2],\n",
              "       [5],\n",
              "       [2],\n",
              "       [1],\n",
              "       [8],\n",
              "       [7],\n",
              "       [9],\n",
              "       [1],\n",
              "       [4],\n",
              "       [5],\n",
              "       [4],\n",
              "       [8],\n",
              "       [2],\n",
              "       [1],\n",
              "       [8],\n",
              "       [5],\n",
              "       [5],\n",
              "       [7],\n",
              "       [5],\n",
              "       [6],\n",
              "       [6],\n",
              "       [9],\n",
              "       [9],\n",
              "       [7],\n",
              "       [3],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [3],\n",
              "       [2],\n",
              "       [7],\n",
              "       [5],\n",
              "       [2],\n",
              "       [4],\n",
              "       [2],\n",
              "       [6],\n",
              "       [4],\n",
              "       [5],\n",
              "       [4],\n",
              "       [4],\n",
              "       [3],\n",
              "       [2],\n",
              "       [9],\n",
              "       [6],\n",
              "       [5],\n",
              "       [3],\n",
              "       [3],\n",
              "       [7],\n",
              "       [3],\n",
              "       [1],\n",
              "       [8],\n",
              "       [7],\n",
              "       [2],\n",
              "       [5],\n",
              "       [5],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [8],\n",
              "       [5],\n",
              "       [4],\n",
              "       [5],\n",
              "       [1],\n",
              "       [5],\n",
              "       [5],\n",
              "       [7],\n",
              "       [5],\n",
              "       [3],\n",
              "       [4],\n",
              "       [3],\n",
              "       [8],\n",
              "       [3],\n",
              "       [8],\n",
              "       [7],\n",
              "       [0],\n",
              "       [7],\n",
              "       [4],\n",
              "       [7],\n",
              "       [9],\n",
              "       [3],\n",
              "       [7],\n",
              "       [8],\n",
              "       [6],\n",
              "       [8],\n",
              "       [4],\n",
              "       [7],\n",
              "       [2],\n",
              "       [6],\n",
              "       [8],\n",
              "       [5],\n",
              "       [6],\n",
              "       [8],\n",
              "       [0],\n",
              "       [4],\n",
              "       [3],\n",
              "       [2],\n",
              "       [6],\n",
              "       [8],\n",
              "       [3],\n",
              "       [2],\n",
              "       [9],\n",
              "       [6],\n",
              "       [8],\n",
              "       [2],\n",
              "       [4],\n",
              "       [7],\n",
              "       [3],\n",
              "       [8],\n",
              "       [1],\n",
              "       [9],\n",
              "       [6],\n",
              "       [2],\n",
              "       [6],\n",
              "       [3],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [5],\n",
              "       [4],\n",
              "       [8],\n",
              "       [5],\n",
              "       [1],\n",
              "       [5],\n",
              "       [3],\n",
              "       [8],\n",
              "       [3],\n",
              "       [0],\n",
              "       [3],\n",
              "       [0],\n",
              "       [4],\n",
              "       [4],\n",
              "       [6],\n",
              "       [8],\n",
              "       [9],\n",
              "       [4],\n",
              "       [0],\n",
              "       [1],\n",
              "       [3],\n",
              "       [2],\n",
              "       [2],\n",
              "       [6],\n",
              "       [4],\n",
              "       [3],\n",
              "       [4],\n",
              "       [1],\n",
              "       [3],\n",
              "       [4],\n",
              "       [5],\n",
              "       [1],\n",
              "       [9],\n",
              "       [0],\n",
              "       [6],\n",
              "       [3],\n",
              "       [8],\n",
              "       [8],\n",
              "       [8],\n",
              "       [4],\n",
              "       [5],\n",
              "       [3],\n",
              "       [1],\n",
              "       [8],\n",
              "       [1],\n",
              "       [9],\n",
              "       [7],\n",
              "       [1],\n",
              "       [2],\n",
              "       [8],\n",
              "       [1],\n",
              "       [7],\n",
              "       [9],\n",
              "       [1],\n",
              "       [1],\n",
              "       [9],\n",
              "       [1],\n",
              "       [8],\n",
              "       [7],\n",
              "       [4],\n",
              "       [2],\n",
              "       [0],\n",
              "       [3],\n",
              "       [8],\n",
              "       [3],\n",
              "       [2],\n",
              "       [1],\n",
              "       [5],\n",
              "       [3],\n",
              "       [4],\n",
              "       [1],\n",
              "       [0],\n",
              "       [6],\n",
              "       [6],\n",
              "       [2],\n",
              "       [1],\n",
              "       [4],\n",
              "       [1],\n",
              "       [8],\n",
              "       [6],\n",
              "       [8],\n",
              "       [0],\n",
              "       [2],\n",
              "       [4],\n",
              "       [5],\n",
              "       [2],\n",
              "       [7],\n",
              "       [5],\n",
              "       [9],\n",
              "       [3],\n",
              "       [0],\n",
              "       [2],\n",
              "       [3],\n",
              "       [7],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [4],\n",
              "       [6],\n",
              "       [8],\n",
              "       [3],\n",
              "       [2],\n",
              "       [3],\n",
              "       [7],\n",
              "       [5],\n",
              "       [1],\n",
              "       [7],\n",
              "       [6],\n",
              "       [3],\n",
              "       [4],\n",
              "       [6],\n",
              "       [5],\n",
              "       [6],\n",
              "       [6],\n",
              "       [6],\n",
              "       [8],\n",
              "       [6],\n",
              "       [2],\n",
              "       [0],\n",
              "       [4],\n",
              "       [0],\n",
              "       [4],\n",
              "       [3],\n",
              "       [8],\n",
              "       [3],\n",
              "       [8],\n",
              "       [1],\n",
              "       [2],\n",
              "       [1],\n",
              "       [6],\n",
              "       [8],\n",
              "       [0],\n",
              "       [8],\n",
              "       [1],\n",
              "       [0],\n",
              "       [7],\n",
              "       [4],\n",
              "       [0],\n",
              "       [4],\n",
              "       [0],\n",
              "       [1],\n",
              "       [7],\n",
              "       [1],\n",
              "       [3],\n",
              "       [7],\n",
              "       [9],\n",
              "       [5],\n",
              "       [6],\n",
              "       [8],\n",
              "       [5],\n",
              "       [3],\n",
              "       [8],\n",
              "       [0],\n",
              "       [7],\n",
              "       [5],\n",
              "       [1],\n",
              "       [7],\n",
              "       [5],\n",
              "       [8],\n",
              "       [2],\n",
              "       [4],\n",
              "       [9],\n",
              "       [7],\n",
              "       [0],\n",
              "       [9],\n",
              "       [9],\n",
              "       [3],\n",
              "       [7],\n",
              "       [5],\n",
              "       [3],\n",
              "       [8],\n",
              "       [5],\n",
              "       [8],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [3],\n",
              "       [1],\n",
              "       [3],\n",
              "       [3],\n",
              "       [6],\n",
              "       [2],\n",
              "       [7],\n",
              "       [7],\n",
              "       [4],\n",
              "       [8],\n",
              "       [9],\n",
              "       [2],\n",
              "       [5],\n",
              "       [8],\n",
              "       [7],\n",
              "       [1],\n",
              "       [6],\n",
              "       [6],\n",
              "       [7],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [8],\n",
              "       [6],\n",
              "       [3],\n",
              "       [1],\n",
              "       [7],\n",
              "       [5],\n",
              "       [3],\n",
              "       [2],\n",
              "       [0],\n",
              "       [7],\n",
              "       [7],\n",
              "       [1],\n",
              "       [1],\n",
              "       [2],\n",
              "       [2],\n",
              "       [9],\n",
              "       [0],\n",
              "       [5],\n",
              "       [5],\n",
              "       [6],\n",
              "       [1],\n",
              "       [5],\n",
              "       [4],\n",
              "       [7],\n",
              "       [8],\n",
              "       [4],\n",
              "       [5],\n",
              "       [8],\n",
              "       [4],\n",
              "       [2],\n",
              "       [6],\n",
              "       [8],\n",
              "       [0],\n",
              "       [1],\n",
              "       [8],\n",
              "       [5],\n",
              "       [5],\n",
              "       [4],\n",
              "       [8],\n",
              "       [6],\n",
              "       [5],\n",
              "       [8],\n",
              "       [4],\n",
              "       [9],\n",
              "       [2],\n",
              "       [0],\n",
              "       [1],\n",
              "       [9],\n",
              "       [9],\n",
              "       [6],\n",
              "       [5],\n",
              "       [6],\n",
              "       [0],\n",
              "       [7],\n",
              "       [4],\n",
              "       [4],\n",
              "       [0],\n",
              "       [2],\n",
              "       [4],\n",
              "       [4],\n",
              "       [0],\n",
              "       [2],\n",
              "       [4],\n",
              "       [0],\n",
              "       [7],\n",
              "       [5],\n",
              "       [4],\n",
              "       [5],\n",
              "       [9],\n",
              "       [4],\n",
              "       [1],\n",
              "       [3],\n",
              "       [3],\n",
              "       [7],\n",
              "       [8],\n",
              "       [3],\n",
              "       [2],\n",
              "       [9],\n",
              "       [2],\n",
              "       [2],\n",
              "       [5],\n",
              "       [7],\n",
              "       [9],\n",
              "       [9],\n",
              "       [5],\n",
              "       [6],\n",
              "       [1],\n",
              "       [8],\n",
              "       [2],\n",
              "       [6],\n",
              "       [3],\n",
              "       [1],\n",
              "       [9],\n",
              "       [2],\n",
              "       [9],\n",
              "       [5],\n",
              "       [2],\n",
              "       [3],\n",
              "       [6],\n",
              "       [3],\n",
              "       [2],\n",
              "       [3],\n",
              "       [2],\n",
              "       [6],\n",
              "       [2],\n",
              "       [6],\n",
              "       [6],\n",
              "       [4],\n",
              "       [5],\n",
              "       [0],\n",
              "       [9],\n",
              "       [0],\n",
              "       [0],\n",
              "       [4],\n",
              "       [2],\n",
              "       [6],\n",
              "       [4],\n",
              "       [3],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [4],\n",
              "       [6],\n",
              "       [7],\n",
              "       [0],\n",
              "       [3],\n",
              "       [3],\n",
              "       [0],\n",
              "       [3],\n",
              "       [6],\n",
              "       [5],\n",
              "       [0],\n",
              "       [4],\n",
              "       [7],\n",
              "       [6],\n",
              "       [0],\n",
              "       [9],\n",
              "       [7],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [5],\n",
              "       [4],\n",
              "       [5],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [4],\n",
              "       [3],\n",
              "       [4],\n",
              "       [0],\n",
              "       [5],\n",
              "       [7],\n",
              "       [2],\n",
              "       [6],\n",
              "       [4],\n",
              "       [5],\n",
              "       [3],\n",
              "       [5],\n",
              "       [2],\n",
              "       [7],\n",
              "       [1],\n",
              "       [0],\n",
              "       [4],\n",
              "       [7],\n",
              "       [3],\n",
              "       [5],\n",
              "       [0],\n",
              "       [7],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [8],\n",
              "       [8],\n",
              "       [4],\n",
              "       [8],\n",
              "       [4],\n",
              "       [2],\n",
              "       [9],\n",
              "       [9],\n",
              "       [3],\n",
              "       [4],\n",
              "       [6],\n",
              "       [9],\n",
              "       [8],\n",
              "       [1],\n",
              "       [5],\n",
              "       [8],\n",
              "       [1],\n",
              "       [8],\n",
              "       [3],\n",
              "       [2],\n",
              "       [8],\n",
              "       [2],\n",
              "       [7],\n",
              "       [3],\n",
              "       [9],\n",
              "       [6],\n",
              "       [6],\n",
              "       [9],\n",
              "       [5],\n",
              "       [7],\n",
              "       [3],\n",
              "       [9],\n",
              "       [0],\n",
              "       [2],\n",
              "       [2],\n",
              "       [1],\n",
              "       [8],\n",
              "       [1],\n",
              "       [0],\n",
              "       [6],\n",
              "       [5],\n",
              "       [9],\n",
              "       [7],\n",
              "       [6],\n",
              "       [0],\n",
              "       [6],\n",
              "       [0],\n",
              "       [8],\n",
              "       [8],\n",
              "       [9],\n",
              "       [2],\n",
              "       [1],\n",
              "       [1],\n",
              "       [5],\n",
              "       [8],\n",
              "       [0],\n",
              "       [3],\n",
              "       [9],\n",
              "       [9],\n",
              "       [9],\n",
              "       [0],\n",
              "       [4],\n",
              "       [7],\n",
              "       [3],\n",
              "       [2],\n",
              "       [0],\n",
              "       [5],\n",
              "       [9],\n",
              "       [3],\n",
              "       [5],\n",
              "       [3],\n",
              "       [1],\n",
              "       [4],\n",
              "       [8],\n",
              "       [1],\n",
              "       [9],\n",
              "       [2],\n",
              "       [4],\n",
              "       [8],\n",
              "       [2],\n",
              "       [9],\n",
              "       [7],\n",
              "       [3],\n",
              "       [6],\n",
              "       [3],\n",
              "       [4],\n",
              "       [5],\n",
              "       [8],\n",
              "       [5],\n",
              "       [7],\n",
              "       [4],\n",
              "       [6],\n",
              "       [3],\n",
              "       [3],\n",
              "       [4],\n",
              "       [3],\n",
              "       [8],\n",
              "       [3],\n",
              "       [2],\n",
              "       [5],\n",
              "       [0],\n",
              "       [9],\n",
              "       [1],\n",
              "       [5],\n",
              "       [3],\n",
              "       [7],\n",
              "       [2],\n",
              "       [8],\n",
              "       [4],\n",
              "       [7],\n",
              "       [8],\n",
              "       [7],\n",
              "       [1],\n",
              "       [8],\n",
              "       [6],\n",
              "       [8],\n",
              "       [8],\n",
              "       [1],\n",
              "       [5],\n",
              "       [5],\n",
              "       [3],\n",
              "       [9],\n",
              "       [0],\n",
              "       [6],\n",
              "       [0],\n",
              "       [6],\n",
              "       [3],\n",
              "       [1],\n",
              "       [5],\n",
              "       [2],\n",
              "       [9],\n",
              "       [2],\n",
              "       [9],\n",
              "       [8],\n",
              "       [6],\n",
              "       [9],\n",
              "       [6],\n",
              "       [8],\n",
              "       [9],\n",
              "       [5],\n",
              "       [4],\n",
              "       [2],\n",
              "       [0],\n",
              "       [0],\n",
              "       [7],\n",
              "       [6],\n",
              "       [6],\n",
              "       [0],\n",
              "       [4],\n",
              "       [5],\n",
              "       [1],\n",
              "       [7],\n",
              "       [8],\n",
              "       [2],\n",
              "       [7],\n",
              "       [6],\n",
              "       [3],\n",
              "       [8],\n",
              "       [6],\n",
              "       [5],\n",
              "       [0],\n",
              "       [1],\n",
              "       [8],\n",
              "       [8],\n",
              "       [9],\n",
              "       [7],\n",
              "       [1],\n",
              "       [5],\n",
              "       [2],\n",
              "       [6],\n",
              "       [3],\n",
              "       [7],\n",
              "       [7],\n",
              "       [0],\n",
              "       [6],\n",
              "       [4],\n",
              "       [4],\n",
              "       [6],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [4],\n",
              "       [9],\n",
              "       [8],\n",
              "       [2],\n",
              "       [9],\n",
              "       [3],\n",
              "       [9],\n",
              "       [8],\n",
              "       [8],\n",
              "       [3],\n",
              "       [9],\n",
              "       [6],\n",
              "       [7],\n",
              "       [1],\n",
              "       [7],\n",
              "       [7],\n",
              "       [3],\n",
              "       [3],\n",
              "       [2],\n",
              "       [5],\n",
              "       [8],\n",
              "       [4],\n",
              "       [0],\n",
              "       [0],\n",
              "       [4],\n",
              "       [8],\n",
              "       [4],\n",
              "       [3],\n",
              "       [4],\n",
              "       [8],\n",
              "       [8],\n",
              "       [7],\n",
              "       [4],\n",
              "       [1],\n",
              "       [9],\n",
              "       [1],\n",
              "       [3],\n",
              "       [2],\n",
              "       [8],\n",
              "       [8],\n",
              "       [2],\n",
              "       [9],\n",
              "       [9],\n",
              "       [7],\n",
              "       [9],\n",
              "       [4],\n",
              "       [9],\n",
              "       [8],\n",
              "       [9],\n",
              "       [0],\n",
              "       [4],\n",
              "       [0],\n",
              "       [4],\n",
              "       [9],\n",
              "       [0],\n",
              "       [9],\n",
              "       [7],\n",
              "       [4],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [6],\n",
              "       [4],\n",
              "       [0],\n",
              "       [4],\n",
              "       [9],\n",
              "       [4],\n",
              "       [8],\n",
              "       [9],\n",
              "       [6],\n",
              "       [1],\n",
              "       [6],\n",
              "       [8],\n",
              "       [4],\n",
              "       [7],\n",
              "       [5],\n",
              "       [6],\n",
              "       [5],\n",
              "       [6],\n",
              "       [5],\n",
              "       [8],\n",
              "       [6],\n",
              "       [5],\n",
              "       [4],\n",
              "       [5],\n",
              "       [5],\n",
              "       [0],\n",
              "       [2],\n",
              "       [8],\n",
              "       [0],\n",
              "       [3],\n",
              "       [5],\n",
              "       [3],\n",
              "       [1],\n",
              "       [5],\n",
              "       [0],\n",
              "       [7],\n",
              "       [5],\n",
              "       [6],\n",
              "       [2],\n",
              "       [0],\n",
              "       [2],\n",
              "       [8],\n",
              "       [1],\n",
              "       [1],\n",
              "       [7],\n",
              "       [5],\n",
              "       [1],\n",
              "       [5],\n",
              "       [5],\n",
              "       [4],\n",
              "       [4],\n",
              "       [6],\n",
              "       [5],\n",
              "       [0],\n",
              "       [2],\n",
              "       [7],\n",
              "       [1],\n",
              "       [7],\n",
              "       [7],\n",
              "       [0],\n",
              "       [8],\n",
              "       [0],\n",
              "       [9],\n",
              "       [1],\n",
              "       [1],\n",
              "       [9],\n",
              "       [8],\n",
              "       [0],\n",
              "       [3],\n",
              "       [9],\n",
              "       [7],\n",
              "       [0],\n",
              "       [6],\n",
              "       [9],\n",
              "       [7],\n",
              "       [6],\n",
              "       [6],\n",
              "       [0],\n",
              "       [8],\n",
              "       [6],\n",
              "       [8],\n",
              "       [7],\n",
              "       [1],\n",
              "       [0],\n",
              "       [6],\n",
              "       [7],\n",
              "       [5],\n",
              "       [1],\n",
              "       [6],\n",
              "       [1],\n",
              "       [6],\n",
              "       [4],\n",
              "       [0],\n",
              "       [9],\n",
              "       [4],\n",
              "       [5],\n",
              "       [9],\n",
              "       [2],\n",
              "       [9],\n",
              "       [6],\n",
              "       [3],\n",
              "       [6],\n",
              "       [3],\n",
              "       [3],\n",
              "       [6],\n",
              "       [4],\n",
              "       [5],\n",
              "       [5],\n",
              "       [0],\n",
              "       [2],\n",
              "       [1],\n",
              "       [9],\n",
              "       [6],\n",
              "       [0],\n",
              "       [8],\n",
              "       [3],\n",
              "       [9],\n",
              "       [6],\n",
              "       [9],\n",
              "       [7],\n",
              "       [2],\n",
              "       [1],\n",
              "       [0],\n",
              "       [3],\n",
              "       [6],\n",
              "       [3],\n",
              "       [7],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [7],\n",
              "       [2],\n",
              "       [4],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYXIi2BGOQRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.array(np.argmax(y_test, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs2zj-atOQRt",
        "colab_type": "text"
      },
      "source": [
        "Finally we can compute the accuracy to see how well our trained network is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbFyALGXOQRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a3fe225-b8d0-4242-ba14-fea51960e9f1"
      },
      "source": [
        "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y_test)]\n",
        "accuracy = sum(correct)/len(correct)\n",
        "print('accuracy = {0}%'.format(accuracy * 100))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 93.30000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp0EbQNdOQRx",
        "colab_type": "text"
      },
      "source": [
        "### Optional exercise: complete the same train and test parts for the regualrized version of the model below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Qu4ObjOQRy",
        "colab_type": "text"
      },
      "source": [
        "### Regularization:\n",
        "\n",
        "We still have one more modification to make to the backprop function - adding regularization to the gradient calculations.  The final regularized version is below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhSJMGMGOQRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(params, input_size, hidden_size1, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):], (num_labels, (hidden_size1 + 1))))\n",
        "    \n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
        "    \n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(theta2.shape)  # (10, 26)\n",
        "    \n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "    \n",
        "    J = J / m\n",
        "    \n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
        "    \n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]  # (1, 401)\n",
        "        z2t = z2[t,:]  # (1, 25)\n",
        "        a2t = a2[t,:]  # (1, 26)\n",
        "        ht = h[t,:]  # (1, 10)\n",
        "        yt = y[t,:]  # (1, 10)\n",
        "        \n",
        "        d3t = ht - yt  # (1, 10)\n",
        "        \n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)\n",
        "        d2t = np.multiply((theta2.T * d3t.T).T, sigmoid_gradient(z2t))  # (1, 26)\n",
        "        \n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + d3t.T * a2t\n",
        "        \n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    \n",
        "    # add the gradient regularization term\n",
        "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learning_rate) / m\n",
        "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * learning_rate) / m\n",
        "    \n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
        "    \n",
        "    return J, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54N1_pTEOQR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b13a0bf0-f827-47f0-aa70-5979492487a4"
      },
      "source": [
        "J, grad = backprop(params, input_size, hidden_size1, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7.186795909823177, (20560,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCXpZ7-EOQR5",
        "colab_type": "text"
      },
      "source": [
        "We're finally ready to train our network and use it to make predictions.  This is roughly similar to the previous exercise with multi-class logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plgVHuHrOQR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "e1127dde-941d-4aa5-f2be-d80bd5449b24"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y_onehot,test_size=0.20,random_state=23)\n",
        "\n",
        "# minimize the objective function\n",
        "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size1, num_labels, X_train, y_train, learning_rate), \n",
        "                method='TNC', jac=True, options={'maxiter': 250})\n",
        "fmin"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     fun: 0.3941434055801533\n",
              "     jac: array([-6.20456306e-05, -1.33990845e-05, -4.89247339e-06, ...,\n",
              "       -1.48921193e-04, -2.50130878e-05, -8.87865063e-04])\n",
              " message: 'Linear search failed'\n",
              "    nfev: 214\n",
              "     nit: 13\n",
              "  status: 4\n",
              " success: False\n",
              "       x: array([-0.36468762, -0.05359634, -0.01956989, ...,  0.03565038,\n",
              "       -1.84800928,  2.59223785])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMsjjCYOQR8",
        "colab_type": "text"
      },
      "source": [
        "We put a bound on the number of iterations since the objective function is not likely to completely converge.  Our total cost has dropped below 0.5 though so that's a good indicator that the algorithm is working.  Let's use the parameters it found and forward-propagate them through the network to get some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dh2ApPzOQR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.matrix(X)\n",
        "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(fmin.x[hidden_size1 * (input_size + 1):], (num_labels, (hidden_size1 + 1))))\n",
        "\n",
        "a1, z2, a2, z3, h = forward_propagate(X_test, theta1, theta2)\n",
        "y_pred = np.array(np.argmax(h, axis=1))\n",
        "y_pred\n",
        "y_test = np.array(np.argmax(y_test, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPRlCVXDOQSA",
        "colab_type": "text"
      },
      "source": [
        "Finally we can compute the accuracy to see how well our trained network is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLN8FOsjOQSB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0aed135-3019-4a1a-d9ea-a5f79ff80193"
      },
      "source": [
        "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y_test)]\n",
        "accuracy = sum(correct)/len(correct)\n",
        "print('accuracy = {0}%'.format(accuracy * 100))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 95.19999999999999%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luJGhHakOQSE",
        "colab_type": "text"
      },
      "source": [
        "And we're done!  We've successfully implemented a rudimentary feed-forward neural network with backpropagation and used it to classify images of handwritten digits.  In the next exercise we'll look at another power supervised learning algorithm, support vector machines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeXL_NUIOQSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}